# Multi-Modal-AI-Travel-Assistant
AI-powered travel assistant that speaks, translates, draws, and books flights

I built a multilingual, voice-powered travel assistant using 
#LLMs
#Frontier Models- OpenAI, Gemini

(Yes, it speaks, translates, draws... and even books your flights.)

ğŸ‘‡ Hereâ€™s the breakdown for both techies and non-techies:

ğŸ§  The *idea*: 
Make flight search more fun, natural, and human. 
â†’ Ask it for prices â†’ it *talks back* 
â†’ Say â€œBook me a ticketâ€ â†’ it *confirms it* 
â†’ Want visuals? â†’ it *draws your destination* 
â†’ Prefer French? â†’ it *translates instantly*

ğŸ’¡ Why these tools? 
Think of each AI model as a specialist in your team:

â†’ **GPT-4** = the brain (handles chat + logic) 
â†’ **DALLÂ·E** = the artist (generates visuals) 
â†’ **TTS (text-to-speech)** = the voice (responds in audio) 
â†’ **Google Gemini** = the translator (switches languages) 
â†’ **Gradio** = the stage (brings it all to life in a browser)

ğŸ› ï¸ Tech Stack: 
â†’ OpenAI APIs (GPT-4, DALLÂ·E, TTS) 
â†’ Google Gemini 
â†’ Python + Gradio

ğŸ§° But hereâ€™s the cool part â€” â€œTOOLSâ€ in LLMs: 

Theyâ€™re not physical tools. 

They're *functions* the model can call when it needs help. 

Think: â€œLet me check the priceâ€ â†’ it uses `get_ticket_price()`. 

Or â€œLet me confirm that bookingâ€ â†’ it calls `make_booking()`.

This is how GPT models go beyond text â€” they can *act*.

ğŸš€ Why this matters: 
We're not just chatting with bots anymore. 
We're interacting with **assistants** that can reason, respond, draw, speak â€” even take actions.

All in one flow. One conversation. One screen.


